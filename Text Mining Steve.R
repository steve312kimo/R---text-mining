# 文字探勘(Text Mining)入門
# 要掌握文字探勘的技術，就必須先從斷詞開始！ 什麼是斷詞？斷詞就是將文章依照詞彙來拆解，這可以幫助我們了解文章是以什麼樣的詞彙所組成，進一步找到高頻詞彙或者主要詞彙。
# 
# 文字探勘的技術早期在國外發展，因此處理英文文章的套件已經發展相當成熟，如tm、tidytext。中文的相關套件則有：結巴(jiebaR), Rwordseg,tmcn。rJava套件對於斷詞相關的套件來說也是必須先安裝的，安裝相關套件的語法如下：
install.packages("rJava")
install.packages("jiebaR")
install.packages("tmcn")
install.packages("Rwordseg")
install.packages("wordcloud")
install.packages("wordcloud2")

library(rJava)
library(tmcn)
library(Rwordseg)
library(dplyr)
library(lubridate)
library(stringr)
library(jiebaR)
library(wordcloud) # 非互動式文字雲
library(wordcloud2) # 互動式文字雲

# jiebaR的說明則 https://rpubs.com/JJChiou/textmining_1
### 1. 如何斷詞？ ------
# 在文字探勘的過程中，一定需要熟悉字串(string)資料的處理方式，但在追求完美以前，先看看jiebaR如何幫我們斷詞，
#以先前在MoneyDJ爬到的新聞為例:

### 系統參數設定 ----
Sys.setlocale(category = "LC_ALL", locale = "zh_TW.UTF-8") # 避免中文亂碼

# 範例文章
content <- "紐約商業交易所（NYMEX）6月原油期貨5月6日收盤上漲0.31美元或0.5%成為每桶62.25美元，因伊朗的局勢升溫，歐洲ICE期貨交易所（ICE Futures Europe）近月布蘭特原油上漲0.39美元或0.6%成為每桶71.24美元。路透社報導，美國正在向中東部署一個航母打擊群和一個轟炸機特遣部隊，美國代理國防部長稱伊朗政權的威脅是可信的。 卡達半島電視台網站5月5日報導，美國本月起取消對8個經濟體（中國、印度、日本、韓國、台灣、土耳其、義大利和希臘）購買伊朗石油的豁免，相比去年11月美國對伊朗石油出口實施制裁的時候允許這些國家在6個月內繼續購買以避免過度影響油價，顯然美國認為如今油市已經有足夠的供應。美國國務卿蓬佩奧（Mike Pompeo）表示，美國已經與主要產油國家進行溝通，希望確保油市的供應充足；加上美國國內的產油也在持續增長，這令美國有信心油市的供應不會匱乏。 不過，實際局勢可能未必如美國所想。目前有多個產油國家內政動盪並影響產量，包括阿爾及利亞、安哥拉、利比亞、伊朗、奈及利亞與委內瑞拉，一旦動盪升級，隨時會進一步影響油市供應。此外，伊朗重質原油也並非任何國家都能替代，遑論美國的輕質原油，與伊朗原油在品質上最為相近的是沙烏地阿拉伯，其次為阿拉伯聯合大公國。而如果油價因為任何供應問題再度飆升至每桶100美元，估計將令全球經濟增長削減0.6個百分點，通膨則將上揚0.7個百分點。 油田服務公司貝克休斯（Baker Hughes Inc.）公佈，截至5月3日，美國石油與天然氣探勘井數量較前週減少1座至990座，創下去年3月（990座）以來的13個月新低。其中，主要用於頁岩油氣開採的水平探勘井數量較前週持平為873座。探勘活動的增減會反映未來的石油產量，貝克休斯統計的探勘井是指為開發以及探勘新油氣儲藏所設的鑽井（鑽機）數量。 貝克休斯的數據顯示，截至5月3日，美國石油探勘井數量較前週所創的逾一年新低增加2座至807座，累計今年來仍減少78座；天然氣探勘井數量較前週減少3座至183座。與去年同期相比，美國石油探勘井數量減少27座，天然氣探勘井數量減少13座；水平探勘井數量年減2座。根據美國能源部週度預測數據，截至4月26日當週，美國原油日均產量再創新高水平至1,230萬桶。 在美國最大產油州德州，油氣探勘井數量較前週減少7座至484座，緊鄰德州上方的奧克拉荷馬州油氣探勘井數量較前週增加1座至103座，新墨西哥州油氣探勘井數量較前週增加2座至106座，路易斯安那州油氣探勘井數量較前週持平為62座，北達科他州油氣探勘井數量較前週減少1座至57座。最大頁岩油產地、盤據西德州與新墨西哥州東南部的二疊紀盆地石油探勘井數量較前週減少1座至459座。"
# 定義斷詞器
cutter <- worker(bylines = F)

# 使用斷詞器斷詞(有兩種寫法)
#segment(content, cutter)
cutter[content]
# [1] "紐約"         "商業"         "交易所"       "NYMEX"        "6"            "月"          
# [7] "原油期貨"     "5"            "月"           "6"            "日"           "收盤"        
# [13] "上漲"         "0.31"         "美元"         "或"           "0.5"          "成為"        
# [19] "每桶"         "62.25"        "美元"         "因"           "伊朗"         "的"      

# 專有名詞被斷開
# 上面有提到，文章是依照詞彙來被斷開，那詞彙的具體內容就必須要先定義清楚，這個定義好的詞庫或是字典，稱之為語料庫(Corpus)。
#一般來說，由於jiebaR已經有內建的語料庫了，所以就算不特別設定也可以斷詞，然而這時候，這樣的結果可能會不符合我們的預期，若預設的語料庫沒有包含我們想關切的「關鍵字」怎麼辦？這時候我們可以加入自定義的詞彙，甚至建立自定義的辭典。
# 
# 以我們剛剛的文章為例，“紐約商業交易所”、“探勘井”以及“頁岩油”等就被斷開而非視為一個詞，因此我們另外將這幾個詞加進語料庫中。

# [1] "紐約"         "商業"         "交易所" 
# [229] "輕質"         "原油"  
# [289] "美國"         "石油"         "與"           "天然氣"       "探勘"         "井"  
# [529] "頁岩"         "油"
new_words <- c("紐約商業交易所","探勘井","頁岩油","輕值原油")
# 一次只能加入一個詞，常常需要搭配迴圈使用
for (i in 1:length(new_words)) {
  new_user_word(cutter, new_words[i])
}

cutter[content]
# [1] "紐約商業交易所" "NYMEX"          "6"              "月"             "原油期貨"      
# [6] "5"              "月"             "6"              "日"             "收盤" 

# 雖然我們可以任意加入自定義的新詞，但以財金領域來說，難道沒有一個財經辭典可以直接安裝在套件上嗎？#
# 當然可以，目前網路上開源的辭典已經包羅萬象，甚至有醫療領域、社會領域等專有詞典提供研究者使用。
# 而tmcn套件本身也提供臺大的情緒辭典(NTUSD)，不用特別上網下載。

### 去掉數字、英文 -----
# 除了專有名詞被斷開以外，還有許多不順眼的數字、甚至英文存在於文章中，也需要先做處理。以下這個方法會用到正規表達式，有興趣的人可以上網搜尋，它在擷取特別格式的文字時非常實用，這個技巧在其他程式語言多半也是通用的。
?str_remove_all

content <- str_remove_all(content, "[0-9a-zA-Z]+?")
cutter[content]
# [1] "紐約商業交易所" "月"             "原油期貨"       "月"             "日"            
# [6] "收盤"           "上漲"           "美元"           "或"             "成為"  

### 去掉冗詞贅字 ----
# 的、然後、於、是、在等贅詞在斷詞時也是一個問題，這些之乎者也對於分析文章的幫助非常有限，而且他們出現的頻率又非常高，jiebaR當然也可以處理。
# 
# 在英文的文字探勘中，停止詞(stop words)的篩選本身也很重要，中文也不例外，我們可以透過外部匯入文件檔(.txt)的方式匯入停止詞(當然，新詞也可以使用匯入的方式)。而停止詞的定義，可以自己隨意定義字串並存成文件檔(.txt)，讓斷詞器匯入，亦或是從網路上下載定義好的停止詞庫，概念跟下載外部的語料庫相同。

# 匯出新詞
new_words <- c("紐約商業交易所","探勘井","頁岩油","輕值原油")
Encoding(new_words)
writeLines(new_words, "new_words.txt")
# 設定停止詞
stop_words <- c("在","的","下","個","來","至","座","亦","與","或","日","月","年","週")
writeLines(stop_words, "stop_words.txt")
# 重新定義斷詞器，匯入停止詞
?worker
# cutter <- worker(user = "new_words.txt", stop_word = "stop_words.txt", bylines = FALSE, detect = T)
# seg_words <- cutter[content]
# seg_words

seg_words <- filter_segment(content, stop_words)
#filter_segment:如果tokens吻合stop_words就移除

cutter[seg_words]

### 2.文字雲(Word Cloud) ----
# 完成了斷詞之後，才是真正的開始，通常第2步驟就是計算詞彙的頻率，通過詞彙的頻率我們就可以直接使用文字雲的套件wordcloud來視覺化文章的重點了！
# 計算詞彙頻率
txt_freq <- freq(seg_words)
# 由大到小排列
txt_freq <- arrange(txt_freq, desc(freq))
# 檢查前5名
head(txt_freq)

# 文字雲套件主要有兩個，wordcloud套件是文字雲的基本款，主要輸出靜態的圖片；wordcloud2顧名思義就是前一個套件的進階版，主要提供互動式的圖片，非常適用在Shiny等網頁中。然而需要注意的是，我認為一般wordcloud的參數比較完整，且兩者參數的命名不盡相同，注意不要混淆了。

par(family=("Microsoft YaHei")) #一般wordcloud需要定義字體，不然會無法顯示中文

# 一般的文字雲 (pkg: wordcloud)
wordcloud(txt_freq$char, txt_freq$freq, min.freq = 2, random.order = F, ordered.colors = F, colors = rainbow(nrow(txt_freq)))

# 互動式文字雲 (pkg: wordcloud2)
wordcloud2(filter(txt_freq, freq > 1), 
           minSize = 2, fontFamily = "Microsoft YaHei", size = 1)
